
# 형태소 분석기
- stemmer : Okt, Komoran, Kkma, mecab 
    - 속도가 중요한 경우에는 Mecab을 쓰는 것이 유리하고, 정규화 기능을 선호한다면 Okt


# mecab 테스트
- 설치방법은 검색한대로 하면됨
```python
import MeCab

m = MeCab.Tagger('-d /usr/local/lib/mecab/dic/mecab-ko-dic')
ret = m.parse('안녕하세요 형태소 분석 부탁드립니다.')

print(ret)
```


#### Mac m1 에서 설치시
```
arch -arm64 brew install cmake
pip install --no-cache-dir sentencepiece
pip install ko-sentence-transformers
```

### 유사어 매칭 
```python
import json
from khaiii import KhaiiiApi
from googletrans import Translator

khaiii = KhaiiiApi()
translator = Translator()

def jaccard_similarity(a, b):
    a, b = set(a), set(b)
    return len(a.intersection(b)) / len(a.union(b))

# 주어진 텍스트(text)를 검사하여 텍스트 내에 아스키 코드 128보다 큰 문자(한국어와 같은 비영어 문자)가 없는 경우에만 번역을 시도합니다.
# 번역을 시도할 때, googletrans 라이브러리의 Translator 클래스를 사용하여 텍스트를 한국어로 번역합니다 (dest='ko'). 번역된 텍스트를 반환합니다.
def translate_to_korean(text):
    try:
        if not any(ord(c) > 128 for c in text):
            return translator.translate(text, dest='ko').text
    except Exception as e:
        print(f"Error while translating text: {e}")
    return text

def tokenize_text(text):
    return [word.lex for sent in khaiii.analyze(text) for word in sent.morphs]

def jsonDecode(input_json):
    try:
        data = json.loads(input_json)
    except json.JSONDecodeError as e:
        print(f"Error while processing data: {e}")
        return {}
    
    return data;

def get_merged_dictionary():
    default_dict = '{"청바지":["진","데님 팬츠","데님 바지"],"빨간색":["빨강","빨간","레드","빨강색"]}'
    custom_dict = '{"난닝구" : ["나시", "난닝구", "민소매"]}'
    default_dict_map = jsonDecode(default_dict)
    custom_dict_map = jsonDecode(custom_dict)
    
    merged_dict = default_dict_map
    for key, values in custom_dict_map.items():
        if key in merged_dict:
            merged_dict[key].extend(values)
        else:
            merged_dict[key] = values

    return merged_dict

def convert_text_using_dictionary(input_value):
    dictionary = get_merged_dictionary()

    if not input_value:
        return input_value

    for key, values in dictionary.items():
        if input_value in values:
            return key

    return input_value

def find_best_match(mp_text, ec_list):
    max_similarity = 0
    best_ec = None

    mp_text_translated = translate_to_korean(mp_text)
    mp_tokenized = tokenize_text(convert_text_using_dictionary(mp_text_translated))

    for ec_text in ec_list:
        ec_text_translated = translate_to_korean(ec_text)
        ec_tokenized = tokenize_text(convert_text_using_dictionary(ec_text_translated))
        similarity = jaccard_similarity(mp_tokenized, ec_tokenized)

        if similarity > max_similarity:
            max_similarity = similarity
            best_ec = ec_text

    return best_ec, max_similarity


def process_data(input_json):
    data = json.loads(input_json)
    results = {}
    ec_remaining = [item for item in data['ec'] if item.strip()]

    for mp_item in data['mp']:
        if mp_item.strip():
            best_ec, max_similarity = find_best_match(mp_item, ec_remaining)
            if max_similarity > 0:
                results[mp_item] = {"match": best_ec, "similarity": max_similarity}
                ec_remaining.remove(best_ec)

    return results


#영어번역은 되나 영문을 소리나는대로 옮긴(로마자 표기) 한글은 안됨
# input_json = '''{
#   "mp": ["갈색", "빨간", "블랙", "블루 블랙", "짙은 초록색", "레드", "로얄 블루", "바이올렛", "터콰이즈", "핑크"],
#   "ec": ["빨강", "01 (Blue Black)", "02 (Brilliant Black)", "03 (Brilliant Brown)", "04 (Brilliant Red)", "05 (Dark Green)", "06 (Pink)", "07 (Royal Blue)", "08 (Turquoise)", "09 (Violet)"]
# }'''

input_json = '{"mp":["진","바지","L(롱다리용)","L(숏다리용)","M(롱다리용)","M(숏다리용)","S(롱다리용)","S(숏다리용)"],"ec":["데님 팬츠","pants","롱 타입 / L","롱 타입 / M","롱 타입 / S","숏 타입 / L","숏 타입 / M","숏 타입 / S"]}'
results = process_data(input_json)
output_json = json.dumps(results, ensure_ascii=False, indent=2)
print(output_json)

```
