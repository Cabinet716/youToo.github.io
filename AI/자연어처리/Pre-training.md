# 사전학습(Pre-training)


## 사전훈련된 워드임베딩
- LSA, 
- Word2Vec, 
- FastText, 
- Glove

## 사전훈련된 언어모델
- ELMo
- BERT는 기존 Transformer에서 Encoder
- GPT는 기존 Transformer에서 Masked Multi-Head Attention 
